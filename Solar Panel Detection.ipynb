{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import packages\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "from tabulate import tabulate\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "from pylab import rcParams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set directory parameters\n",
    "'''\n",
    "# Set the directories for the data and the CSV files that contain ids/labels\n",
    "dir_train_images  = 'data/training/'\n",
    "dir_test_images   = 'data/testing/'\n",
    "dir_train_labels  = 'data/labels_training.csv'\n",
    "dir_test_ids      = 'data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_data, dir_labels, training = True):\n",
    "    '''\n",
    "    Load each of the image files into memory\n",
    "    \n",
    "    When training = True, the labels are also loaded\n",
    "    '''\n",
    "    labels_pd = pd.read_csv(dir_labels)\n",
    "    ids       = labels_pd.id.values\n",
    "    data      = []\n",
    "    \n",
    "    for identifier in ids:\n",
    "        fname     = dir_data + identifier.astype(str) + '.tif'\n",
    "        image     = mpl.image.imread(fname)\n",
    "        data.append(image)\n",
    "        \n",
    "    data = np.array(data) # Convert to Numpy array\n",
    "    \n",
    "    if training:\n",
    "        labels = labels_pd.label.values\n",
    "        return data, labels\n",
    "    \n",
    "    else:\n",
    "        return data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_extract_features(data, prc):\n",
    "\n",
    "    if prc == 'color':\n",
    "        # Filter images to keep only colors in the optimal range found at the end of this notebook\n",
    "        \n",
    "        color_lo = np.asarray([0, 0, 110])   # Lower color\n",
    "        color_hi = np.asarray([55, 87, 187]) # Higher color\n",
    "        new_data = []                        # Empty list with new features (average color values for RGB channels)\n",
    "        \n",
    "        for img in data:\n",
    "            # Iterate over every image in set\n",
    "            \n",
    "            img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "            mask = cv2.inRange(img_hsv, color_lo, color_hi) \n",
    "            mask[mask != 0] = 1                             \n",
    "            mask = np.expand_dims(mask, axis = 2)\n",
    "            mask = np.append(\n",
    "                np.append(mask, mask, axis = 2), \n",
    "                mask, \n",
    "                axis = 2\n",
    "            )\n",
    "            feature = img * mask                            \n",
    "            \n",
    "            if np.count_nonzero(feature) != 0: \n",
    "                \n",
    "                \n",
    "                feature = np.sum(np.sum(feature, axis = 0), axis = 0) / np.count_nonzero(feature)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                feature = [255, 255, 255]\n",
    "\n",
    "            new_data.append(feature)                        \n",
    "        \n",
    "        features = np.array(new_data)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    elif prc == 'HOG':\n",
    "        \n",
    "        \n",
    "        get_hog = lambda X: hog(X, orientations = 9, pixels_per_cell = (16, 16),\n",
    "                                cells_per_block = (4, 4), visualize = False,channel_axis=-1)\n",
    "        features = np.array([get_hog(image) for image in data])\n",
    "\n",
    "        return features\n",
    "    \n",
    "    elif prc == 'NA':\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_classifier(clf):\n",
    "    '''\n",
    "    Shared function to select the classifier for both performance evaluation and testing\n",
    "    '''\n",
    "    if clf == 'knn':\n",
    "        return KNeighborsClassifier(n_neighbors = 7)\n",
    "    \n",
    "    elif clf == 'logistic':\n",
    "        return LogisticRegression(solver = 'liblinear')\n",
    "    \n",
    "    elif clf == 'dummy':\n",
    "        return DummyClassifier(strategy = 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_performance_assessment(X, y, k, clf, prc):\n",
    "    \n",
    "    prediction_scores = np.empty(y.shape[0], dtype = 'object')\n",
    "    kf = StratifiedKFold(n_splits = k, shuffle = True)\n",
    "    \n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        \n",
    "        X_train_l, X_val = X[train_index], X[val_index]\n",
    "        y_train_l        = y[train_index]\n",
    "        X_train_features = preprocess_and_extract_features(X_train_l, prc)        \n",
    "        clf = clf.fit(X_train_features, y_train_l)\n",
    "        X_val_features = preprocess_and_extract_features(X_val, prc)\n",
    "        cpred = clf.predict_proba(X_val_features)\n",
    "        prediction_scores[val_index] = cpred[:, 1]\n",
    "        \n",
    "    return prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(labels, prediction_scores, legend, color):\n",
    "    \n",
    " \n",
    "    fpr, tpr, _   = metrics.roc_curve(labels, prediction_scores, pos_label = 1)\n",
    "    auc           = metrics.roc_auc_score(labels, prediction_scores)\n",
    "    legend_string = legend + ' (AUC = {:0.3f})'.format(auc)  \n",
    "    plt.plot(fpr, tpr, label = legend_string, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prc(labels, prediction_scores, legend, color):\n",
    "   \n",
    "    precision, recall, thresholds = precision_recall_curve(labels, prediction_scores)\n",
    "    average_precision = average_precision_score(labels, prediction_scores)\n",
    "    legend_string = legend + ' (AP = {:0.3f})'.format(average_precision)  \n",
    "    plt.plot(recall, precision, label = legend_string, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(labels, prediction_scores):\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels, prediction_scores)\n",
    "    i                   = np.arange(len(tpr))\n",
    "    roc                 = pd.DataFrame(\n",
    "        {'tf': pd.Series(tpr - (1 - fpr), index = i), 'threshold': pd.Series(threshold, index = i)}\n",
    "    )\n",
    "    roc_t               = roc.iloc[(roc.tf - 0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train = load_data(dir_train_images, dir_train_labels)\n",
    "X_test, ids_test = load_data(dir_test_images, dir_test_ids, training = False)\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0 = set_classifier('dummy')\n",
    "clf0.fit(X_train, y_train)\n",
    "prediction_scores0 = clf0.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_folds1 = 10\n",
    "clf1 = set_classifier('knn')\n",
    "prc1 = 'color'\n",
    "prediction_scores1 = cv_performance_assessment(X_train, y_train, num_training_folds1, clf1, prc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_folds2 = 10\n",
    "clf2 = set_classifier('knn')\n",
    "prc2 = 'HOG'\n",
    "prediction_scores2 = cv_performance_assessment(X_train, y_train, num_training_folds2, clf2, prc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train = np.stack((prediction_scores1, prediction_scores2), axis = -1)\n",
    "num_training_folds3 = 10\n",
    "clf3 = set_classifier('logistic')\n",
    "prc3 = 'NA'\n",
    "prediction_scores3 = cv_performance_assessment(Z_train, y_train, num_training_folds3, clf3, prc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('ROC Curves')\n",
    "plt.plot([0, 1], [0, 1], '--', color = 'gray', label = 'Chance')\n",
    "\n",
    "\n",
    "plot_roc(y_train, prediction_scores0, legend = 'Random Guessing', color = 'darkgray')\n",
    "plot_roc(y_train, prediction_scores1, legend = 'Model 1', color = '#61d4b3')\n",
    "plot_roc(y_train, prediction_scores2, legend = 'Model 2', color = '#fdd365')\n",
    "plot_roc(y_train, prediction_scores3, legend = 'Model 3', color = '#fb8d62')\n",
    "#plot_roc(y_train, prediction_scores4, legend = 'Model 4', color = '#fd2eb3')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('Precision-Recall Curves')\n",
    "\n",
    "plot_prc(y_train, prediction_scores0, legend = 'Random Guessing', color = 'darkgray')\n",
    "plot_prc(y_train, prediction_scores1, legend = 'Model 1', color = '#61d4b3')\n",
    "plot_prc(y_train, prediction_scores2, legend = 'Model 2', color = '#fdd365')\n",
    "plot_prc(y_train, prediction_scores3, legend = 'Model 3', color = '#fb8d62')\n",
    "#plot_prc(y_train, prediction_scores4, legend = 'Model 4', color = '#fd2eb3')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split              = 0.75\n",
    "X_train2, y_train2 = X_train[:int(X_train.shape[0] * split)], y_train[:int(X_train.shape[0] * split)]\n",
    "X_val, y_val       = X_train[int(X_train.shape[0] * split):], y_train[int(X_train.shape[0] * split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf0v = set_classifier('dummy')\n",
    "clf0v.fit(X_train2, y_train2)\n",
    "prediction_scores0v = clf0v.predict(X_val)\n",
    "prediction_scores0t = clf0v.predict(X_train2)\n",
    "cutoff0       = get_threshold(y_val, prediction_scores0v)\n",
    "predictions0v = np.copy(prediction_scores0v)\n",
    "print('Model 0 optimal cuttof is {}'.format(cutoff0[0]))\n",
    "pd.DataFrame(confusion_matrix(y_val, predictions0v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1v              = set_classifier('knn')\n",
    "prc1v              = 'color'\n",
    "X_train_features1v = preprocess_and_extract_features(X_train2, prc1v)\n",
    "X_val_features1v   = preprocess_and_extract_features(X_val,    prc1v)\n",
    "clf1v.fit(X_train_features1v, y_train2)\n",
    "prediction_scores1v = clf1v.predict_proba(X_val_features1v)[:, 1]\n",
    "prediction_scores1t = clf1v.predict_proba(X_train_features1v)[:, 1]\n",
    "cutoff1                                 = get_threshold(y_val, prediction_scores1v)\n",
    "predictions1v                           = np.copy(prediction_scores1v)\n",
    "predictions1v[predictions1v <= cutoff1] = 0\n",
    "predictions1v[predictions1v > cutoff1]  = 1\n",
    "print('Model 1 optimal cuttof is {}'.format(cutoff1[0]))\n",
    "pd.DataFrame(confusion_matrix(y_val, predictions1v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2v              = set_classifier('knn')\n",
    "prc2v              = 'HOG'\n",
    "X_train_features2v = preprocess_and_extract_features(X_train2, prc2v)\n",
    "X_val_features2v   = preprocess_and_extract_features(X_val,    prc2v)\n",
    "clf2v.fit(X_train_features2v, y_train2)\n",
    "prediction_scores2v = clf2v.predict_proba(X_val_features2v)[:, 1]\n",
    "prediction_scores2t = clf2v.predict_proba(X_train_features2v)[:, 1]\n",
    "cutoff2                                 = get_threshold(y_val, prediction_scores2v)\n",
    "predictions2v                           = np.copy(prediction_scores2v)\n",
    "predictions2v[predictions2v <= cutoff2] = 0\n",
    "predictions2v[predictions2v > cutoff2]  = 1\n",
    "print('Model 2 optimal cuttof is {}'.format(cutoff2[0]))\n",
    "pd.DataFrame(confusion_matrix(y_val, predictions2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_val              = np.stack((prediction_scores1v, prediction_scores2v), axis = -1)\n",
    "Z_train2           = np.stack((prediction_scores1t, prediction_scores2t), axis = -1)\n",
    "clf3v              = set_classifier('logistic')\n",
    "clf3v.fit(Z_train2, y_train2)\n",
    "prediction_scores3v = clf3v.predict_proba(Z_val)[:, 1]\n",
    "cutoff3                                 = get_threshold(y_val, prediction_scores3v)\n",
    "predictions3v                           = np.copy(prediction_scores3v)\n",
    "predictions3v[predictions3v <= cutoff3] = 0\n",
    "predictions3v[predictions3v > cutoff3]  = 1\n",
    "print('Model 3 optimal cuttof is {}'.format(cutoff3[0]))\n",
    "pd.DataFrame(confusion_matrix(y_val, predictions3v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting ROC Curves for Performance on Validation Set\n",
    "'''\n",
    "# Compute and plot the ROC curves\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('ROC Curves')\n",
    "plt.plot([0, 1], [0, 1], '--', color = 'gray', label = 'Chance')\n",
    "\n",
    "# Plot ROC for each model\n",
    "plot_roc(y_val, prediction_scores0v, legend = 'Random Guessing', color = 'darkgray')\n",
    "plot_roc(y_val, prediction_scores1v, legend = 'Model 1', color = '#61d4b3')\n",
    "plot_roc(y_val, prediction_scores2v, legend = 'Model 2', color = '#fdd365')\n",
    "plot_roc(y_val, prediction_scores3v, legend = 'Model 3', color = '#fb8d62')\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting PRC Curves for Performance on Validation Set\n",
    "'''\n",
    "# Compute and plot the PRC curves\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.title('Precision-Recall Curves')\n",
    "\n",
    "# Plot ROC for each model\n",
    "plot_prc(y_val, prediction_scores0v, legend = 'Random Guessing', color = 'darkgray')\n",
    "plot_prc(y_val, prediction_scores1v, legend = 'Model 1', color = '#61d4b3')\n",
    "plot_prc(y_val, prediction_scores2v, legend = 'Model 2', color = '#fdd365')\n",
    "plot_prc(y_val, prediction_scores3v, legend = 'Model 3', color = '#fb8d62')\n",
    "\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid('on')\n",
    "plt.axis('square')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data, extract features, and train the classifier on the training data\n",
    "training_features1 = preprocess_and_extract_features(X_train, prc1)\n",
    "clf1.fit(training_features1, y_train)\n",
    "\n",
    "# Load the test data and test the classifier\n",
    "test_features1  = preprocess_and_extract_features(X_test, prc1)\n",
    "test_scores1    = clf1.predict_proba(test_features1)[:, 1]\n",
    "\n",
    "# Save the predictions to a CSV file \n",
    "submission_file = pd.DataFrame({'id':    ids_test,\n",
    "                                'score': test_scores1})\n",
    "submission_file.to_csv('data\\sample_submission.csv',\n",
    "                       columns = ['id', 'score'],\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data, extract features, and train the classifier on the training data\n",
    "training_features2 = preprocess_and_extract_features(X_train, prc2)\n",
    "clf2.fit(training_features2, y_train)\n",
    "\n",
    "# Load the test data and test the classifier\n",
    "test_features2  = preprocess_and_extract_features(X_test, prc2)\n",
    "test_scores2    = clf2.predict_proba(test_features2)[:, 1]\n",
    "\n",
    "# Save the predictions to a CSV file \n",
    "submission_file = pd.DataFrame({'id':    ids_test,\n",
    "                                'score': test_scores2})\n",
    "submission_file.to_csv('data\\sample_submission.csv',\n",
    "                       columns = ['id', 'score'],\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data, extract features, and train the classifier on the training data\n",
    "training_features3 = Z_train\n",
    "clf3.fit(training_features3, y_train)\n",
    "\n",
    "# Load the test data and test the classifier\n",
    "# Create test features Z with predictions scores from Model 1 and Model 2\n",
    "Z_test = np.stack((test_scores1, test_scores2), axis = -1)\n",
    "test_features3  = Z_test\n",
    "test_scores3    = clf3.predict_proba(test_features3)[:, 1]\n",
    "\n",
    "# Save the predictions to a CSV file \n",
    "submission_file = pd.DataFrame({'id':    ids_test,\n",
    "                                'score': test_scores2})\n",
    "submission_file.to_csv('data\\sample_submission.csv',\n",
    "                       columns = ['id', 'score'],\n",
    "                       index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_colors(img, color_lo, color_hi):\n",
    "    '''\n",
    "    Function to test the color filtering method\n",
    "    '''\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(img_hsv, color_lo, color_hi)\n",
    "    #mask[mask == 0]  = 1 #filter out\n",
    "    #mask[mask != 1]  = 0\n",
    "    mask[mask != 0] = 1 #keep colors\n",
    "    mask = np.expand_dims(mask, axis = 2)\n",
    "    mask = np.append(np.append(mask, mask, axis = 2), mask, axis = 2)\n",
    "    features = img * mask\n",
    "    #features = np.mean(np.mean(features, axis = 0), axis = 0)\n",
    "    \n",
    "    if np.count_nonzero(features) != 0: \n",
    "        features = np.sum(np.sum(features, axis = 0), axis = 0) / np.count_nonzero(features)\n",
    "    else:\n",
    "        features = [0, 0, 0]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loop to calculate AUC for KNN models with different color filters\n",
    "'''\n",
    "aucs      = []\n",
    "color_los = []\n",
    "color_his = []\n",
    "\n",
    "for r_lo in range(0, 1, 1):\n",
    "    for r_hi in range(55, 66, 5):\n",
    "        for g_lo in range(0, 1, 1):\n",
    "            for g_hi in range(87, 98, 5):\n",
    "                for b_lo in range(105, 116, 5):\n",
    "                    for b_hi in range(187, 198, 5):\n",
    "                        color_lo = np.asarray([r_lo, g_lo, b_lo])\n",
    "                        color_hi = np.asarray([r_hi, g_hi, b_hi])\n",
    "                        features = np.array([extract_colors(img, color_lo, color_hi) for img in X_train])\n",
    "                        \n",
    "                        knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "                        knn.fit(features, y_train)\n",
    "                        scores = knn.predict_proba(features)[:, 1]\n",
    "                        auc = metrics.roc_auc_score(y_train, scores)\n",
    "                        \n",
    "                        color_los.append((r_lo, g_lo, b_lo))\n",
    "                        color_his.append((r_hi, g_hi, b_hi))\n",
    "                        aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimal colors approx.\n",
    "'''\n",
    "color_los[aucs.index(max(aucs))], color_his[aucs.index(max(aucs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Random indexes for each class\n",
    "'''\n",
    "zeros = X_train[y_train == 0] # No solar panel\n",
    "ones  = X_train[y_train == 1] # Solar panel\n",
    "\n",
    "i_zeros = np.random.choice(zeros.shape[0], 25, replace = False)\n",
    "i_ones  = np.random.choice(ones.shape[0],  25, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--------------------CLASSIFYING THE IMAGE WITH SOLAR PANELS AND IMAGES WITHOUT SOLAR PANELS-------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images with solar panels\n",
    "'''\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "plt.figure()\n",
    "plt.suptitle('Images with solar panels')\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    plt.imshow(ones[i_ones[i]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample images with no solar panels\n",
    "'''\n",
    "rcParams['figure.figsize'] = 16, 3\n",
    "plt.figure()\n",
    "plt.suptitle('Images with no solar panels')\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(1, 25, i + 1)\n",
    "    plt.imshow(zeros[i_zeros[i]])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ones = len(ones)\n",
    "count_zeros = len(zeros)\n",
    "total_count = count_ones + count_zeros\n",
    "\n",
    "data = [[\"With Solar Panel\", count_ones], [\"Without Solar Panel\", count_zeros], [\"Total\", total_count]]\n",
    "table = tabulate(data, headers=[\"Category\", \"Count\"], tablefmt=\"grid\")\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
